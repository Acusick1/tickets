# Ticket Scraper

## Agent instructions

Important: this file is read-only, do not edit this file!

- Do not create markdown documents (other than README.md) without permission. If you would like to write additional documents, finish the rest of your tasks first and make the case to the user.

## Project Setup with uv

```bash
# Install uv (if not already installed)
curl -LsSf https://astral.sh/uv/install.sh | sh

# Initialize project
uv init tickets
cd tickets

# Add dependencies
uv add playwright sqlalchemy apscheduler flask pyyaml
uv add --dev pytest pytest-asyncio pytest-mock

# Install Playwright browsers
uv run playwright install chromium

# Create necessary directories
mkdir -p config data tests
```

## Configuration Files

**config/alerts.yaml** - User-defined alerts:
```yaml
alerts:
  - name: "Event Name"
    source: "ticketmaster"  # or "stubhub", "viagogo"
    url: "https://..."
    target_price: 150.00
    active: true
```

**config/settings.yaml** - Application settings:
```yaml
email:
  smtp_host: "smtp.gmail.com"
  smtp_port: 587
  sender_email: "your-email@gmail.com"
  sender_password: "your-app-password"
  recipient_email: "your-email@gmail.com"

scraping:
  interval_minutes: 15
  timeout_seconds: 30
  headless: true
  user_agent: "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
```

## Database Schema

**alerts table:**
- id, name, source, source_url, target_price
- last_notified_price (nullable - tracks last notification)
- is_active, last_checked, created_at

**price_records table:**
- id, alert_id (FK), price, availability, timestamp, raw_data (JSON)

**notification_log table:**
- id, alert_id (FK), sent_at, trigger_reason, price

**Relationships:**
- Alert has many PriceRecords
- Alert has many NotificationLogs# Ticket Price Scraper - Implementation Instructions

## Project Overview
Build a single-user Python system that scrapes ticket prices from Ticketmaster, StubHub, and Viagogo every 15 minutes, stores price history, and sends email notifications when prices drop below user-defined thresholds.

## Requirements Summary

### Functional Requirements
- Single user with multiple concurrent price alerts
- Scrape every 15 minutes per alert
- Email notification when price drops below threshold
- Continue notifying if price continues to drop (not if stays same)
- Store complete price history
- Optional lightweight web dashboard for viewing price trends

### Technical Stack
- **Package Management**: uv (fast Python package installer)
- **Scraping**: Playwright (handles JavaScript-heavy sites)
- **Scheduling**: APScheduler (in-process scheduling)
- **Database**: SQLite with SQLAlchemy ORM
- **Email**: smtplib (built-in SMTP)
- **Dashboard**: Flask + Chart.js (optional)
- **Config**: YAML file for alert definitions
- **Testing**: pytest with class-based tests, fixtures, and conftest

## Core Components

### Database Models
- Alert model: tracks event URLs, price thresholds, notification state
- PriceRecord model: stores historical price data with timestamps
- NotificationLog model: tracks when and why notifications were sent
- Relationships: Alert has many PriceRecords and NotificationLogs

### Scrapers
- Abstract base scraper with Playwright initialization
- Site-specific implementations for Ticketmaster, StubHub, Viagogo
- Each scraper returns standardized format: {price, availability, raw_data}
- Built-in retry logic, error handling, and anti-detection measures

### Alert Manager
- Orchestrates scraping across all active alerts
- Evaluates notification conditions
- Coordinates between scrapers, database, and notifier

### Scheduler
- APScheduler configuration for periodic scraping
- Job persistence across restarts
- Graceful shutdown handling

### Notifier
- SMTP email sending with templated messages
- Tracks notification history in database
- Handles SMTP errors gracefully

### Dashboard (Optional)
- Flask web interface for viewing alerts
- Chart.js visualization of price history
- Minimal styling, focus on functionality

## Notification Logic (Critical)

```python
# Pseudo-code for notification decision
current_price = scrape_result['price']
target_price = alert.target_price
last_notified = alert.last_notified_price

if current_price < target_price:
    if last_notified is None:
        # First time below threshold
        send_notification()
        alert.last_notified_price = current_price
    elif current_price < last_notified:
        # Price continuing to drop
        send_notification()
        alert.last_notified_price = current_price
    # else: price same or increased, no notification
```

## Scraping Best Practices

1. **Anti-Detection:**
   - Use headless=False initially for testing
   - Rotate user agents
   - Add random delays (2-5 seconds)
   - Use Playwright stealth plugin if available

2. **Error Handling:**
   - Wrap scraping in try-except
   - Log all errors with traceback
   - Take screenshots on failure
   - Continue with other alerts if one fails

3. **Rate Limiting:**
   - Don't scrape same URL more than once per 15 minutes
   - Add jitter to scheduled times (±2 minutes random)

4. **Debugging:**
   - Save raw HTML on first successful scrape
   - Log all selector matches
   - Verbose logging during development

## Testing Strategy

### Test Structure
Use pytest with class-based tests organized by component:

```python
# tests/conftest.py - Shared fixtures
@pytest.fixture
def db_session():
    """Provide test database session"""
    
@pytest.fixture
def sample_alert():
    """Create test alert instance"""
    
@pytest.fixture
def mock_scraper():
    """Mock scraper for testing without network calls"""

# tests/test_models.py
class TestAlert:
    def test_create_alert(self, db_session):
        """Test alert creation"""
        
    def test_alert_relationships(self, db_session, sample_alert):
        """Test price_records and notifications relationships"""

# tests/test_scrapers.py
class TestBaseScraper:
    def test_initialization(self):
        """Test scraper setup"""
        
class TestTicketmasterScraper:
    def test_scrape_success(self, mock_scraper):
        """Test successful scraping"""
        
    def test_scrape_sold_out(self, mock_scraper):
        """Test sold-out detection"""

# tests/test_alert_manager.py
class TestAlertManager:
    def test_process_alerts(self, db_session, sample_alert, mock_scraper):
        """Test alert processing logic"""
        
    def test_notification_logic(self, db_session, sample_alert):
        """Test notification decision making"""

# tests/test_notifier.py
class TestNotifier:
    def test_send_email(self, mock_smtp):
        """Test email sending"""
```

### Test Fixtures in conftest.py
- `db_session`: In-memory SQLite database for tests
- `sample_alert`: Pre-configured alert for testing
- `sample_price_records`: Historical price data
- `mock_playwright`: Mock Playwright browser
- `mock_smtp`: Mock SMTP server for email tests
- `test_config`: Test configuration overrides

### Running Tests
```bash
# Run all tests
uv run pytest

# Run with coverage
uv run pytest --cov=src --cov-report=html

# Run specific test class
uv run pytest tests/test_scrapers.py::TestTicketmasterScraper

# Run with verbose output
uv run pytest -v
```

## Running the Application

```bash
# Configure alerts and settings first
# Edit config/alerts.yaml and config/settings.yaml

# Run application
uv run python main.py

# Run dashboard (optional, separate terminal)
uv run python -m src.dashboard.app

# Run tests
uv run pytest
```

## Testing Approach

1. **Test scrapers individually** - Use mock responses, test selector logic
2. **Test with headless=False** - Watch browser automation to debug selectors
3. **Test notification logic** - Use fixtures with various price scenarios
4. **Test database operations** - Use in-memory SQLite for fast tests
5. **Test email** - Mock SMTP server, verify email content
6. **Integration tests** - Test full flow with mocked external dependencies

## Common Issues & Solutions

**Issue: Playwright detected as bot**
- Use stealth plugin: `playwright-stealth`
- Slow down actions with delays
- Use real browser profile instead of headless

**Issue: Selectors not finding elements**
- Element may load dynamically - add wait_for_selector
- Check if element is in iframe
- Use Playwright inspector: `playwright codegen <url>`

**Issue: Gmail blocking SMTP**
- Use App Password (not regular password)
- Enable "Less secure app access" (not recommended)
- Switch to SendGrid API

**Issue: Database locked errors**
- Use `check_same_thread=False` in SQLite connection
- Consider connection pooling
- Close sessions properly

## Security Notes

- **Never commit** settings.yaml with real credentials
- Use environment variables for sensitive data in production
- Add to `.gitignore`: `config/settings.yaml`, `data/`, `.venv/`, `*.pyc`, `__pycache__/`, `.pytest_cache/`, `htmlcov/`, `.coverage`
- For Gmail, use App Passwords, not account password

## Success Criteria

The implementation is complete when:
1. All pytest tests pass
2. System scrapes configured sites every 15 minutes without crashes
3. Email notifications sent correctly when prices drop
4. Price history stored and queryable in database
5. Dashboard (if implemented) shows price trends accurately
6. System runs continuously for 24+ hours without intervention

---

**Implementation order: Database models → Scrapers → Alert manager → Scheduler → Tests → Dashboard (optional)**